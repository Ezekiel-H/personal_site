[{"content":"Presentation code Overview This presentation takes two datasets provided and leveraging both snowflake and DBT demonstrates the benefits of pivoting to a modern platform. Additionally I have introduced a presentation layer using both Gitab and R to further touch on the flexiblity of using a rapid development.\nThe presentation is outlines in the following way\n Approach: Data Results A/B Testing Discussion points  Approach DBT Not wanting to reinvent the wheel I followed the standard user guide here focusing on ensuring a reproducible pattern was established.\nDatabase structure I decided on a two step approach for the database structure where the initial data was inserted in its entirety into the RAW/BIKES layer then I constructed a \u0026ldquo;Raw Data Store\u0026rdquo; RAW/RDS that was to be used as the source layer.\nThis was to provide clear and definable separation between working in the snowflake UI and within a development environment in DBT. It also has a number of associated benefits:\n Assuming you wanted to automate the ingestion it would protect downstream dependencies json handling DBT mapping Developer friendly documentation  RAW ----/BIKES ---------/CITI_BIKES ---------/WEATHER ----/RDS --------/RDS_BIKES --------/RDS_BIKES_SAMPLE --------/RDS_WEATHER --------/RDS_WEATHER_SAMPLE ANALYTICS ---------/ADS -------------/BIKES_METRICS -------------/RIDER_WEATHER -------------/WEATHER_METRICS With the plan as above DBT will pass the data from as\nSnowflake The snowflake database then takes the shape as follows below with an initial setup piece and establishing connections to the S3 buckets that is captured with a few scripts.\n s3 connections setup  Gitlab The codebase is captured here. As it is a single developer working on the project I set up a simple structure of allowing for direct code commits and pushes against a feature branch and self approval into main.\nEnvironmentally there is no consideration to promotion of code through environments but structurally the separation could be introduced by extending the native dbt_user protections and moving the jobs onto protected branches.\nData The raw data was located in two sets under the database raw:\nbikes: https://s3.amazonaws.com/tripdata/index.html\nweather: s3://snowflake-workshop-lab/weather-nyc\nBikes Taking a sample in R to prepare the load we were left with the following structure\ncreate or replace table citi (tripduration integer, starttime timestamp, stoptime timestamp, start_station_id integer, start_station_name string, start_station_latitude float, start_station_longitude float, end_station_id integer, end_station_name string, end_station_latitude float, end_station_longitude float, bikeid integer, membership_type string, usertype string, birth_year integer, gender integer); Looking a little closer (after some googling) I found there was a path declared online for the s3 bucket in the following format.AsIs(\nURL s3://snowflake-workshop-lab/citibike-trips This allowed me to input all 61 million rows\u0026hellip; which may have been excessive for the exercise but I digress.\nWeather The weather dataset followed the exact same pattern apart from leveraging a standard JSON pattern. Which snowflake lets us deal with really cleanly\n{ \u0026quot;city\u0026quot;: { \u0026quot;coord\u0026quot;: { \u0026quot;lat\u0026quot;: 43.000351, \u0026quot;lon\u0026quot;: -75.499901 }, \u0026quot;country\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;findname\u0026quot;: \u0026quot;NEW YORK\u0026quot;, \u0026quot;id\u0026quot;: 5128638, \u0026quot;langs\u0026quot;: [ { \u0026quot;abbr\u0026quot;: \u0026quot;NY\u0026quot; }, { ... }, { \u0026quot;zh\u0026quot;: \u0026quot;纽约州\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;New York\u0026quot;, \u0026quot;zoom\u0026quot;: 1 }, \u0026quot;clouds\u0026quot;: { \u0026quot;all\u0026quot;: 90 }, \u0026quot;main\u0026quot;: { \u0026quot;humidity\u0026quot;: 81, \u0026quot;pressure\u0026quot;: 1016, \u0026quot;temp\u0026quot;: 285.53, \u0026quot;temp_max\u0026quot;: 286.15, \u0026quot;temp_min\u0026quot;: 285.15 }, \u0026quot;time\u0026quot;: 1478206834, \u0026quot;weather\u0026quot;: [ { \u0026quot;description\u0026quot;: \u0026quot;overcast clouds\u0026quot;, \u0026quot;icon\u0026quot;: \u0026quot;04d\u0026quot;, \u0026quot;id\u0026quot;: 804, \u0026quot;main\u0026quot;: \u0026quot;Clouds\u0026quot; } ], \u0026quot;wind\u0026quot;: { \u0026quot;deg\u0026quot;: 280, \u0026quot;speed\u0026quot;: 6.2 } } All I actually want from here is weather so lets get temp, wind speed, and the weather\nResults Now with all the data loaded into snowflake via DBT I wanted to connect something that would be able to display it in a meaningful way. So creating a connection into R I made a few graphics to understand the posed questions.\nI wanted to use highcharter so I connected the snowflake instance to R for more fluid datacleaning ahead of presentation\ninstall.packages(c(\u0026quot;DBI\u0026quot;, \u0026quot;dplyr\u0026quot;,\u0026quot;dbplyr\u0026quot;,\u0026quot;odbc\u0026quot;, \u0026quot;Rcpp\u0026quot;)) library(DBI) library(dplyr) library(dbplyr) library(odbc) library(Rcpp) con \u0026lt;- DBI::dbConnect(odbc::odbc(), Driver = \u0026quot;/opt/snowflake/snowflakeodbc/lib/universal/libSnowflake.dylib\u0026quot;, Server = \u0026quot;jm03779.ap-southeast-2.snowflakecomputing.com\u0026quot;, UID = rstudioapi::askForPassword(\u0026quot;Database user\u0026quot;), PWD = rstudioapi::askForPassword(\u0026quot;Database password\u0026quot;), Database = \u0026quot;Analytics\u0026quot;, Warehouse = \u0026quot;compute_wh\u0026quot;, Schema = \u0026quot;ads\u0026quot; ) DBI::dbGetQuery(con,\u0026quot;USE ROLE TRANSFORMER;\u0026quot;) DBI::dbGetQuery(con,\u0026quot;USE WAREHOUSE TRANSFORMING;\u0026quot;) base_bike \u0026lt;- DBI::dbGetQuery(con,\u0026quot;SELECT * FROM ANALYTICS.ADS.BIKES_METRICS;\u0026quot;) base_weather \u0026lt;- DBI::dbGetQuery(con,\u0026quot;SELECT * FROM ANALYTICS.ADS.WEATHER_METRICS;\u0026quot;) base_join \u0026lt;- DBI::dbGetQuery(con,\u0026quot;SELECT * FROM ANALYTICS.ADS.RIDER_WEATHER;\u0026quot;) #totals base_bike %\u0026gt;% na.omit() %\u0026gt;% summarise(sum(TRIPS_COUNT), sum(DISTANCE_TOTAL), mean(DURATION_AVERAGE)) Number of trips The total number of trips over the period was 61,468,359\n Distance travelled The total distance traveleld was (rounded for obvious reasons) 111,215,593. This was achieved using the haversine function as hinted in snowflake.\n{{ config(materialized='table', database='analytics') }} -- this aggregates by hour the metrics of distance, time and totals select date_trunc('hour', starttime) as date , count(*) as trips_count , sum(tripduration) as duration_total , avg(tripduration) as duration_average , sum(haversine( start_station_latitude , start_station_longitude , end_station_latitude , end_station_longitude )) as distance_total , avg(haversine( start_station_latitude , start_station_longitude , end_station_latitude , end_station_longitude )) as distance_average from {{ source('rds', 'rds_bikes') }} group by 1  Average trip duration Now this is a bit more interesting of a question. In terms of average duration the average of what? Each trip? The dataset was aggregated at the hour layer so the presentation layer passes a DURATION_AVERAGE column which is the average of averages. Which is 916.8 Trip Duration (seconds)\nA/B testing Trying to understand if weather has an impact on ridership we need to combine the datasets and run an analysis overtop of it.\nThe approach I have decided to go for is using R to run a linear regression and checking for significance.\nThe data We had two datasets weather and bikes data. I decided to aggregate the weather into hourly buckets and took the most prevaling type, for example if there is 2 occurances of rain and one of clear it would call the hour rain.\n-- this assigns a weather for any given hour, could add tempature, wind etc if needed select weather , date from ( select weather , date_trunc('hour', time) as date , count(*) as n , row_number() over (partition by date order by n desc) rank_number from {{ source('rds', 'rds_weather') }} group by 1, 2 ) where rank_number = 1 Now I aggregated the riders into hour buckets as well and joined up the dataset. So question remains does the weather impact the riders.\nSo we have the following weather which I then grouped into \u0026lsquo;good\u0026rsquo; and \u0026lsquo;bad\u0026rsquo;:\n WEATHER WEATHER_TYPE 1 \u0026quot;Clear\u0026quot; \u0026quot;Good\u0026quot; 2 \u0026quot;Mist\u0026quot; \u0026quot;Bad\u0026quot; 3 \u0026quot;Clouds\u0026quot; \u0026quot;Good\u0026quot; 4 \u0026quot;Rain\u0026quot; \u0026quot;Bad\u0026quot; 5 \u0026quot;Haze\u0026quot; \u0026quot;Bad\u0026quot; 6 \u0026quot;Thunderstorm\u0026quot; \u0026quot;Bad\u0026quot; 7 \u0026quot;Fog\u0026quot; \u0026quot;Bad\u0026quot; 8 \u0026quot;Drizzle\u0026quot; \u0026quot;Bad\u0026quot; 9 \u0026quot;Snow\u0026quot; \u0026quot;Bad\u0026quot; 10 \u0026quot;Smoke\u0026quot; \u0026quot;Bad\u0026quot; 11 \u0026quot;Squall\u0026quot; \u0026quot;Bad\u0026quot; Here we just want to look at the weather as there are only 24 hours in the day to get a sense of is the weather good or bad in NYC? Also doing this highlighed a data quality issue in the analysis and narrowed the date range down to 2017-03-01.\n So now lets consider the two averages are there more trips when its good weather?\n It looks like it does. Now to validate this I would need to construct a boolean variable off of this and check its significance as an explanation. Though I am going to stop right here.\nDiscussion  Separating DBT loading and running function to deliver a more stable solution. Using a full code based solution you can see and replicate 100% of what I have completed. Native connections into R, Python will allow a positioning as a \u0026lsquo;data plumber\u0026rsquo; and appeal to the new world.  ","permalink":"http://ezekiel.nz/presentation/firn/","summary":"Presentation code Overview This presentation takes two datasets provided and leveraging both snowflake and DBT demonstrates the benefits of pivoting to a modern platform. Additionally I have introduced a presentation layer using both Gitab and R to further touch on the flexiblity of using a rapid development.\nThe presentation is outlines in the following way\n Approach: Data Results A/B Testing Discussion points  Approach DBT Not wanting to reinvent the wheel I followed the standard user guide here focusing on ensuring a reproducible pattern was established.","title":"FIRN presentation"},{"content":"All figures are made up for illustrative purposes A project I am proud of is rolling out a strategy to send fraud rejected traffic to 3DS unlocking 40+ million in benefit from otherwise declined good customers. Why am I proud of this, well it worked. Until it didn\u0026rsquo;t. Righting the ship was a challenge and ultimately the solution highlighted what was great about this project.\nAgenda So what are we going to talk about today?\n Background, what is fraud? what is the product vision? Why TARR, why now? Exploring the idea, using GOAL Roll out and delegation Unforeseen risks righting the ship Reflections  Background and what is fraud? Given you have given me up to an hour to present I\u0026rsquo;ve taken the liberty to take my time and show you how to commit fraud.\nWhat is payments fraud anyway?\nWith 100s of millions at risk a year we would want a solution to stop it.\n One solution is to stop all the fraud, by stopping all payments\u0026hellip; so the ongoing challenge is balancing fraud risk while approving good customers.  So our product vision is all about improving payment performance.\n The team is then tasked with constantly trying to move the needle on the cost of fraud metric, while products, markets and strategy moves.  So now we know what fraud is, and why does it matters lets talk about the TARR project.\n  TARR So how can you move the needle along, improve good traffic being accepted or block more fraud. Looking across the payment flow we see opportunities and risks.\n We can improve the model accept by investments in ML or new/improved features. Or our outsourced 3rd party vendor can be pushed to improve performance. Continuous improvement aside, how do we come up with creative (read good) ideas?\nStaying involved in what matters, discussions with vendors, colleges and the business, watching the movements of the industry and competitors. Weekly discussions across internal payments teams, platform, accommodations operations allows us to better understand where we are what blockers are there and have been moved.\nSo right before TARR where were we?\n COVID had lowered traffic substantially (outside impact). Riskified (our 3rd party vendor) was being pushed to preform better, within expectations. Our model was preformant and gains slower (continuous improvement). Operations had limited ability to find or optimize features. Platform had recently migrated all SCA flows allowing for better visibility into 3DS.  This last point was the basis for trying out a new approach to unlock the good customers in declined bucket. 3DS was historically a black box SCA forced an improvement of the infrastructure increasing visibility. We had stable fraud rates and due to COVID on balance the risk was higher of rejecting good traffic.\nWe considered what if we just sent all the rejected traffic to 3DS? Terrible idea. But could we make it a good one.\n Outside of the gain from moving the needle along we were targeting.\n Customer experience, customers otherwise rejected would now have a path to book. Lowering impact on customer service and partner service outbound. Increasing authorizations on traffic supporting payments strategic goals.    TARR using the GAME structure.  Goal Release a new pipeline to unlock customers otherwise rejected for fraud. Action Construct and monitor an ability to send rejected customers to 3DS.  Identify and mitigate segments where 3DS is not available. Ensure 3DS flow was being triggered. Ensure there was an opt out option. Appropriate flags to be passed and documented between teams. Monitoring ongoing and actionable.   Metrics  High level (Weekly splits)  Fraud rate. Traffic. Product value (Unblocked commission less fraud).   Detailed level (Daily splits)  Growth by \u0026lsquo;segment\u0026rsquo;. Liability 3DS enrollment 3DS pass rates (proxy).     Evaluate Evaluate if the metric could be a false positive indicator  Fraud maturation. Unique vs non unique. Payments experiments and outside influence.    Mapping stakeholders We wanted to consider the impact the project would have on various teams the level of commitment required and how aligned with product goals it was. Touching on some of the key teams points of consideration are listed below.\nSo how?, how did we involve the rest of the company and ensured their buy in. Trust. Delivered, detailed, data backed trust.\nCompleting a joint proposal document, then reviewed within fraud. Later presented to payments and platform PMs. Iterating over there feedback we signed off and then road mapped.\nRoll out and delegation. Rolling out the project it was a matter of connecting the various parts of the roadmap ensuring blockers were removed and the different stakeholders were kept informed. For example confirming with the payments strategy team on PSPs that supported 3DS, passing this to operations to define as a source of truth and create a datatable for it that can be consumed by the developers. s\nThen moving less hands on the senior analyst now solely accountable for continued monitoring, implementation of mitigation and communication with the development teams to progress the project incrementally to 100% roll out.\nUnforeseen risk and righting the ship 4 - 6 weeks later. Early issues began to emerge. The fraud rate was spiking.\n The analyst investigating the situation explained the issue. Fraud was being sent to 3DS yes. But it was sometimes \u0026ldquo;enrolled while shopping\u0026rdquo;\u0026quot; passing the liability back to us.\nAssessing the situation:\n What was the problem  The missing liability flag when combined with the low overall traffic meant it was difficult for the operations team to spot emerging trends and mitigate the risk.   We had a short term fix with the specific behavioral pattern blocked.  No operational ability to mitigate over time. The project was no longer viable without the liability flag.   Payments track did not have this feature available and was unlikely in this quarter to be able to shift focus.  Taking quick actions\n Short term scale back from 50% to 5%. Discussions quickly upward to send early signal that project might be killed. Opened discussion with our PSP on issue, presented with option to buy off the shelf a solution that can be implemented on the specific call. Opened new analysis on potential value of the project with added data.  Righting the ship The ship was sinking but it turned out to be a very valuable one, and caused a shift in priorities from its genesis.\nGiven the flow was an MVP the roll out of rules needed to be pushed from a developer. With the increasing demands on operations analysts we re prioritized an integration of the existing rules framework.\nThe analysis of the potential gains if we could leverage a liability flag drove the conversation into payments. The project had provided a real increasing in unblocked traffic. This let us drive the acquisition of the off the shelf service and release a payments resource to support the API call.\nIn 1 week we had a turn around on off shelf solution. Allowing us to re trigger to 50% re delegate scale and monitor. Adding in a new monitoring metric for liability declines.\nThe ownership and communication still established the pivot empowered the success of the project into the midterm. The current projected annual value of the project is estimated at 40M + annually. With current efforts across platform to establish the product for other verticals.\n Reflections Defining the risks early and getting stakeholder engagement.\nMisunderstanding the risk of the missing flag.\nChallenges for operational monitoring outside normal analyst flow.\nComplacency of success and false metrics.\nLeveraging vendor/psp to bootstrap solutions\nTARR at Vend?? The project was focused on improving the authorizations and moving the north star metric. Well Vend is not the merchant on record, so does not face these challenges. Though I would gather your customers care and who knows what the future holds there is massive opportunity in owning the payments funnel.\nWhat I hope this project highlights that I do think would support success at Vend.\n Developing new ideas ontop of existing frameworks How I engage and get buy-in from your stakeholders and collaborate both in engineering and design (in this case payments strategy) to find the best solutions. How I can define evaluate and review metrics. How I can fail, learn and pivot.  A final point is maybe where is Vend going? When you consider the likes of Booking, Uber, Amazon all applying for payment service licenses the impact of owning that payment flow has immense benefits at scale. One big risk I see is Stripe a payments first pos provider.\n","permalink":"http://ezekiel.nz/presentation/tarr/","summary":"All figures are made up for illustrative purposes A project I am proud of is rolling out a strategy to send fraud rejected traffic to 3DS unlocking 40+ million in benefit from otherwise declined good customers. Why am I proud of this, well it worked. Until it didn\u0026rsquo;t. Righting the ship was a challenge and ultimately the solution highlighted what was great about this project.\nAgenda So what are we going to talk about today?","title":"Project TARR"},{"content":"","permalink":"http://ezekiel.nz/post/2021-04-26-graph-networks-and-fraud/","summary":"","title":"Graph networks and fraud"},{"content":"In creating my 90 day plan I leant on the overall mission of the data function at Transpower which can be summarised succinctly as \u0026ldquo;Enhancing decision making\u0026rdquo;. Then taking assumptions from previous conversations, I feel I have a solid understanding of where the organisation wants to be in 3 years time.\n  Self servicing\n  Automated tests, alerting, pipelines and deployments\n  Predictive analytics\n  Data as a service\n  Integration of DevOps as a practice\n  The challenge: In 90 days, in a new role, in a new team, how can we deliver success?\n Building foundations while delivering milestone achievements Taking predictive analytics as an example:\n To have predictive analytics we need to have a prediction model serving production data  In order to have that we need to have the framework for production deployment  In order to have that we need to have consistent data and tooling to unlock the ability of individuals build out these frameworks      So a foundation building block is providing a consistent working environment that is industry standard, unblocking the analytics function. I propose this is a juypter server. It does not give us predictive analytics but a milestone achievement along the path to predictive analytics.\n Prioritization framework Considering the above, how should we define what can and should be achieved by this role in the 90 day time frame? Leveraging a simple Effort vs Impact quadrant the 90 plan focuses on quick wins and laying foundations for strategic innovation.\n 90 days into 6 cycles Splitting out the 90 days into three key focus areas:\n GIT and workflows Data and tooling Stakeholder engagement  We are aiming to achieve milestones of\n Sprint cycles Deployment and production tests Code review Jupyter application for analytics Data pipelines alerting and monitoring    On to the presentation\n","permalink":"http://ezekiel.nz/archive/90-day-plan/","summary":"In creating my 90 day plan I leant on the overall mission of the data function at Transpower which can be summarised succinctly as \u0026ldquo;Enhancing decision making\u0026rdquo;. Then taking assumptions from previous conversations, I feel I have a solid understanding of where the organisation wants to be in 3 years time.\n  Self servicing\n  Automated tests, alerting, pipelines and deployments\n  Predictive analytics\n  Data as a service","title":"90 day plan"},{"content":"When I can I try to use R so when looking to put a personal site together a few years back I thought it was a good time to explore the blogdown package. Using this I was able to build and maintain the site using mostly R, GIT and a good grasp of how to google css/html.\nI thought I would detail a step by step how to follow this to create a barebones copy and how to host it (for free) on GITHUB\u0026rsquo;s pages\nWhy do this So couple things firstly why? In the modern age an online presence is more of a conclusion than an option so it\u0026rsquo;s importanat to control the narrative. Sure you can use Medium or LinkdIn or an off the shelf website build but fuck that, thats lame.\nA second consideration is why GIT and blogdown, well first off its free and secondly the blogdown structure is designed mobile first, SEO optimised and with the option to write in RMD lets you move seamlessly between code (Not just R) and text. I personally enjoy playing with data and code and R is my first love so to speak so its nice to combine the two.\nDisclamir If you are happy to work it out for yourself there is a very detailed guide in the docs https://bookdown.org/yihui/blogdown/get-started.html\nOverview  This guide will show step by step how to set up a blogdown site on GIT. Setting up R Setting up GIT Picking a theme Local to Production cycle  Setting up R My folder path is ~R/ so creating a new folder example_ezekiel.nz on ~/R/example_ezekiel.nz and creating a fresh project in the folder, R and install the packages blogdown.\ninstall.packages('blogdown') To quote Yihis \u0026lsquo;Since blogdown is based on the static site generator Hugo (https://gohugo.io), you also need to install Hugo. There is a helper function in blogdown to download and install it automatically on major operating systems (Windows, macOS, and Linux):'\nblogdown::install_hugo() Then simply\nblogdown::new_site() If you are getting an error check that your wd is on your folder path in my example ~R/example_ezekiel.nz/ and that the folder is empty.\nsetwd(\u0026quot;C:/Users/Ezekiel/Desktop/R/example_ezekiel.nz/\u0026quot;) I was getting some wild errors turns out that my R was still on version 3.30 and some packages were failing. Guess 2016 is too long between personal package updates.\nAssuming its all ok you should get the following\n\u0026gt; Want to serve and preview the site now? (y/n) Hit y and you will be set with the basic site and template on your local host. So now looking in my folder I have the following.\ncontent/ layouts/ R/ reasources/ static/ themes/ .Rprofile config.yaml index.Rmd netlify.toml I can use the following commands to see the blogdown default site.\nblogdown::build_site() blogdown::serve_site() This should render down a new folder /public which we will use as part of the deployment flow. The way I would advise is to create a folder /docs and copy the public folder across after rendering the build.\nAwesome simple as that. So now on to GIT.\nGIT Following the GIT pages guide you need to sign up for an account and create a repo. The one for this site is called personal_site as an example but I will continue with the example_ezekiel.nz repo.\nGo to the folder where you want to store your project, and initiate the repo with\ngit init git add . git commit -m \u0026quot;initial commit\u0026quot; git remote add origin git@github.com:Ezekiel-H/example_ezekiel git push -u origin master Now in the settings change the source to docs/\nAdjust the source to /docs and you should be done now at https://ezekiel-h.github.io/example_ezekiel.nz/ you should have a functional site.\n","permalink":"http://ezekiel.nz/post/using-r-and-git-to-host-a-pesonal-site/","summary":"When I can I try to use R so when looking to put a personal site together a few years back I thought it was a good time to explore the blogdown package. Using this I was able to build and maintain the site using mostly R, GIT and a good grasp of how to google css/html.\nI thought I would detail a step by step how to follow this to create a barebones copy and how to host it (for free) on GITHUB\u0026rsquo;s pages","title":"Using R and GIT to host a free pesonal site"},{"content":"Bitcoin cryptocurrency is treated as both a currency and more and more an asset that is exchanged across multiple FX across the globe. This presents an interesting question does the market balance the FX across currencies in real time? If not there would be room for arbitrage and a real ability to make money transferring between currencies using BTC markets.\nThis quick analysis will look to see if it is theoretically possible to make money changing NZD for BTC then to GBP and back to NZD via a BTC exchange. Firstly lets look at the price of bitcoin relative to GBP and NZD over its recent history.\n Well two things are very quickly apparent\n  There is a significant number of gaps in the data provided to quandl by BitcoinLocal that may influence the significance of the result.\n  The price movements are not one for one for the two currencies creating in theory arbitrage opportunities.\n  This means in theory we could buy use NZD to buy BTC transfer into GBP wallet then use a service like coinbase to \u0026ldquo;cash out\u0026rdquo; and use a regular currency transfer service like Azamo to then transfer back into NZD.\n  Here we can see the historic NZD/GBP exchange sourced from the BOE dataset on quandl against the daily BTCExchange understood as the price in BTC GBP vs BTC NZD.\n  The Details So about now you realize that if it was this easy to do it the arbitrage opportunities would shut the door and fees decimate your returns. Well that is true but not entirely assuming the formula below.\n$$ \\Pi = \\frac{X}{(XE - F_t)} (XE_{BTC}-F_{BTC}) (XE_{MT}) - X $$\nWhere profit is the amount invested X divided by the exchange less fees where the fees $F_t$ and $F_{ETC}$ are the unknown. The above chart is a visual representation of when you have the opportunity for arbitrage by trading NZD.\nTaking a practical approach assuming fees are no more than 1% per transaction how much could have been made on the 8th of November per transaction completed transaction. Using the current Azimo transfer as $XE_{MT}$\n$$ $39.88 = \\frac{$100}{((1.8925) (.99))} ((1.3083)(.99))(1.982) - $100 $$\nThis shows with a $100 investment each completed move from NZD to NZD via currency exchange in BTC would have returned a 39.88% return.\nSo what now? How much are fees? How many trades can I make? How much can I make? All goood questions I don\u0026rsquo;t know the answer to. This requires a better dataset than I currently have access to. The point remains that in theory day trading currencies using BTC could be very lucrative.\n","permalink":"http://ezekiel.nz/post/using-btc-exchange-to-find-currency-arbitrage/","summary":"Bitcoin cryptocurrency is treated as both a currency and more and more an asset that is exchanged across multiple FX across the globe. This presents an interesting question does the market balance the FX across currencies in real time? If not there would be room for arbitrage and a real ability to make money transferring between currencies using BTC markets.\nThis quick analysis will look to see if it is theoretically possible to make money changing NZD for BTC then to GBP and back to NZD via a BTC exchange.","title":"Using BTC exchange to find currency arbitrage"},{"content":"In New Zealand to distribute alcohol you need to apply to your regional Liqour Licence Authority (LLA) for a permit. The NZ Justice Department produces a list of all liqour licences distributed in a single year.\nI made this shiny app using a data set from NZ Justice Department and a shapefile/population statistics from Statistics to show for each LLA how many persons per bar, club or liqour retailer. This can be understood as bars per capita for NZ grouped by LLA\u0026rsquo;s.\n Further work planned on this app is to add a drill down function, regional statistics and to account for age with the R18 drinking age.\n","permalink":"http://ezekiel.nz/post/looking-at-the-density-of-bars-accross-nz-lla-s/","summary":"In New Zealand to distribute alcohol you need to apply to your regional Liqour Licence Authority (LLA) for a permit. The NZ Justice Department produces a list of all liqour licences distributed in a single year.\nI made this shiny app using a data set from NZ Justice Department and a shapefile/population statistics from Statistics to show for each LLA how many persons per bar, club or liqour retailer. This can be understood as bars per capita for NZ grouped by LLA\u0026rsquo;s.","title":"Bars per capita in NZ"},{"content":"Christchurch is New Zealand\u0026rsquo;s second largest city and in the 2017 election cycle it was a key prize for any of the political parties. I produced a series of interactive graphics to examine the outcome of Christchurch\u0026rsquo;s current and past three election cycles. The analysis was built using data sourced from data.govt.nz and elections.org.nz. The boundries of some of the Christchurch electorates reach beyond the city limits but for the sake of this assessment the electorates were kept as a whole.\nA bit of quick background - New Zealand has 120(most of the time) sitting members of parliment with 64 general electorate seats (63 in 2008 and 2011) and 7 Maori electorate seats. This leaves a total of 49 seats to be allocated proportionally by the party vote once the allocation of electorate seats is accounted for. A good breakdown of the New Zealand electoral seats can be found in Spinoffs Article here.\nWithin Christchurch there are 7 electorates so the residents elect 7 MP\u0026rsquo;s. Christchurch\u0026rsquo;s enrolled population within these electoral districts represents roughly 10.5% of the NZ enrolled population. Bluntly speaking this means Christchurch in affect elects another 5 MP\u0026rsquo;s from the party list. In reality it\u0026rsquo;s not that simple but the real affect is so minor it will not be elaborated on here (yet).\nAll the graphs are interactive and you can take off any of the parties/year by using clicking the relevant parties/year.\n ##Election Graphs\n Looking at the party vote in Christchurch grouping the dataset by year and party. This graph shows Christchurch is a land of the major parties with National, Labour, Greens and NZ First taking up the lions share of the vote. Between 2008 and 2011 we see the most dramatic shift from Labour to National while Greens and NZF continue to make ground into 2014. The minor parties show Act has become all but irrelavant since 2008\u0026rsquo;s high.\n Grouped by year and electorate this graph shows the winning candidate of each electorate along with the margin of victory by votes. Notice Selywn and Christchurch East are pretty safe seats while the Waimakariri, Christchuch Central and Port Hills are all up for grabs. In 2011 Christchurch Central was decided by only 47 votes.\n ##Christchurch Region Shiny App Two shapefiles stats.nz\u0026rsquo;s NZ electorate maps and LINZ\u0026rsquo;s urban area maps were combined using ARC before being read into R. The Shiny app displays an interactive visualisation for Electorate and Party votes over the previous three elections.\n  ##Christchurch Demographics\nAll these assessments seem pretty straight forward but what about the 5% party vote threshold affecting the relevent party vote? And does New Zealand\u0026rsquo;s younger demographic and better enrollment impact the outcome?\nMaybe. I ran out of time to get this put together before the election so below you can see Christchurch vs NZ age distribution for both enrolled and represented voters. It shows Christchurch has a higher electorate turnout and a younger distribution than the general NZ population. This graph also demonstrates how impactful the unenrolled 18-24 group has on distorting the age demographic in New Zealand.\n ","permalink":"http://ezekiel.nz/post/christchurch-election/","summary":"Christchurch is New Zealand\u0026rsquo;s second largest city and in the 2017 election cycle it was a key prize for any of the political parties. I produced a series of interactive graphics to examine the outcome of Christchurch\u0026rsquo;s current and past three election cycles. The analysis was built using data sourced from data.govt.nz and elections.org.nz. The boundries of some of the Christchurch electorates reach beyond the city limits but for the sake of this assessment the electorates were kept as a whole.","title":"Christchurch Election"},{"content":"          My name is Zeke I am from New Zealand based in Amsterdam. I am interested in data, history, politics and the environment. Totally an R and Shiny nerd but spend most of my time working in Python.\nMostly this site is an excuse to write some Shiny apps and as a place potential recruiters can see that I have enough ability to run blogdown::build_site() I am excited to learn how I can help with charitable or open source projects especially focused on sustainability and environmental causes, please reach out to me on linkedIn.\n {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addProviderTiles\",\"args\":[\"CartoDB.Positron\",null,null,{\"errorTileUrl\":\"\",\"noWrap\":false,\"detectRetina\":false}]},{\"method\":\"addMarkers\",\"args\":[52.3667,4.8946,null,null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Zekes in Amsterdam\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]}],\"limits\":{\"lat\":[52.3667,52.3667],\"lng\":[4.8946,4.8946]}},\"evals\":[],\"jsHooks\":[]} ","permalink":"http://ezekiel.nz/about/","summary":"My name is Zeke I am from New Zealand based in Amsterdam. I am interested in data, history, politics and the environment. Totally an R and Shiny nerd but spend most of my time working in Python.\nMostly this site is an excuse to write some Shiny apps and as a place potential recruiters can see that I have enough ability to run blogdown::build_site() I am excited to learn how I can help with charitable or open source projects especially focused on sustainability and environmental causes, please reach out to me on linkedIn.","title":"Ezekiel Haggart"},{"content":"","permalink":"http://ezekiel.nz/search/","summary":"search","title":"Search"}]